Moloch is [Scott Alexander](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/)'s fill-in for misaligned incentives. Multipolar traps (traps where each agent pursuing what's beneficial for them will be harmful for society at large)) will occur no matter what you do.
- Everyone who takes one less day off will be slightly more respectable than their coworkers, but this ends up in everyone having to take days off. 
- The more a politician lies, the higher chances they'll end up in office.
- Tons of good examples on the post.

The only way to fix the traps of game theory is to change the game/enforce better restrictions. Alexander extends this to talk about how technically a monarchy is the best way to overcome this. A monarch is (theoretically) the person who is most above incentive-seeking. Game theory doesn't apply because they don't need to play games anymore. But, as Alexander says, then you run into the problem of the monarch being terrible. So he has to extend one step further. What if you could design a monarchy where the monarch was benign, all-knowing, etc. This is AI. He wants AI to come and be our savior against Moloch. An ideal AI will design the perfect civilization where there's no incentives and everything is perfect. 

My quibble with this: I think some problems are intractable no matter how much intelligence you throw at them. Some people talk about the coming of AI like this utopia that will be perfect and everything. AI will be a perfect ruler and everyone will be happy and live fulfilling and meaningful lives. First of all, the relatively low probabilities of AI alignment working out well is a massive barrier to this. I think the chances of all-out extinction are slightly overstated, but I think that the threat posed by unaligned AI is probably an even-worse and consistently-accelerating version of nuclear bombs. But assuming AI goes well, people are basically assuming that these problems aren't intractable. If you throw enough intelligence and resources at them, you can solve them. I'm not sure about that. One man's utopia is another man's dystopia (I don't know who that's from). I don't think there can ever be a utopia for everyone. Our desires and patterns of action are completely controlled by what we internalize throughout our life process. The only way for everyone to have the same beliefs and desires (which would be the only way for us to have a perfect utopia) would be for everyone to have the **exact same** life. The exact same. There could be no randomness, no nothing. Everything has to be perfect. And I think the implications of that would probably be dystopian. 

